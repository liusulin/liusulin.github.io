---
title: Scalable optimization for ML
type : 
date: '2022-11-21'
---
## Scalable optimization for ML

I have developed methods that scale up multi-task learning under the  distributed/federated setting [3]. Also, I have worked on developoing and analyzing theoretical properties of gradient-based optimization algorithms for multi-task learning [4], distributed optimization [2], and matrix factorization [1]. 

#### Related Publications

[1] **Revisiting the Landscape of Matrix Factorization**. [Paper](http://proceedings.mlr.press/v108/valavi20a.html) | [Video](https://slideslive.com/38930097/revisiting-the-landscape-of-matrix-factorization?ref=search-presentations-Revisiting+the+Landscape+of+Matrix+Factorization)\
Hossein Valavi, <ins>Sulin Liu</ins>, Peter J. Ramadge. *International Conference on Artificial Intelligence and Statistics* (AISTATS), 2020. 

[2] **Communication-Efficient Distributed Primal-Dual Algorithm for Saddle Point Problems**. [Paper](http://auai.org/uai2017/proceedings/papers/286.pdf)\
Yaodong Yu*, <ins>Sulin Liu</ins>* (equal contr.), Sinno Jialin Pan. *Conference on Uncertainty in Artificial Intelligence* (UAI), 2017. 


[3] **Distributed Multi-Task Relationship Learning**. [Paper](https://arxiv.org/abs/1612.04022) | [Video](https://www.youtube.com/watch?v=az3jbBl-zXI)\
<ins>Sulin Liu</ins>, Sinno Jialin Pan, Qirong Ho. *SIGKDD Conference on Knowledge Discovery and Data Mining* (KDD), 2017. 

[4] **Adaptive Group Sparse Multi-task Learning via Trace Lasso**. [Paper](https://www.ijcai.org/Proceedings/2017/328)\
<ins>Sulin Liu</ins>, Sinno Jialin Pan. *International Joint Conference on Artificial Intelligence* (IJCAI), 2017.